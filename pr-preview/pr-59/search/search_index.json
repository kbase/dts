{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Overview","text":"<p>The Data Transfer Service (DTS) is a web service that handles requests for file transfers between participating organizations interested in exchanging data. The DTS coordinates provides a single point of access for these organizations, allowing an end user or another service to</p> <ul> <li>search for datasets / files within any participating organization based on   criteria specified in an ElasticSearch   style query</li> <li>select any or all files from a search and request a transfer from the source   organization to another participating organization</li> </ul> <p>DTS is designed for easy deployment and maintenance behind a gateway that provides TLS/SSL encryption. Requests to the DTS include headers with authentication information, so these requests rely on the HTTPS protocol to protect this information.</p> <p>It's very easy to deploy the DTS in a Docker environment and configure it using environment variables.</p>"},{"location":"index.html#contents","title":"Contents","text":"<ul> <li>Administrator Guide: How to configure and deploy the DTS</li> <li>Integration Guide: How to hook your organization's   database up to the DTS to take advantage of its capabilities</li> <li>Developer Guide: A detailed description of the DTS   code structure and important concepts</li> </ul>"},{"location":"admin/index.html","title":"DTS Administrator Guide","text":"<p>This guide walks you through the process of installing and configuring the DTS, both locally and in a Docker-based environment. It also discusses some technical issues that arise in granting the DTS permissions to filesystems and other services it needs to perform data transfers.</p>"},{"location":"admin/index.html#contents","title":"Contents","text":"<ul> <li>Installing DTS Locally</li> <li>Deploying DTS via Docker</li> <li>Configuring DTS</li> <li>Granting DTS Access to a Globus Endpoint</li> </ul>"},{"location":"admin/config.html","title":"Configuring the DTS","text":"<p>You can configure a DTS instance by creating a YAML text file similar to dts.yaml.example in the repository. Typically this file is named <code>dts.yaml</code>, and is passed as an argument to the <code>dts</code> executable. Here we describe the different sections in this file and how they affect your DTS instance.</p>"},{"location":"admin/config.html#configuration-file-sections","title":"Configuration File Sections","text":"<p>Click on any of the links below to see the relevant details for a section.</p> <ul> <li>service: configure\u0455 settings for the DTS web service   such as the port on which it listens, the maximum number of connections,   intervals for polling and scrubbing completed tasks, data directories, and   diagnostics</li> <li>endpoints: configures the endpoints used to transfer   files from one place to another</li> <li>databases: configures databases for organizations that   integrate with the DTS</li> </ul> <p>Each of these sections is described below, with a motivating example.</p>"},{"location":"admin/config.html#service","title":"<code>service</code>","text":"<pre><code>service:\n  port: 8080\n  max_connections: 100\n  poll_interval:   60000\n  endpoint: globus-local\n  data_dir: /path/to/dir\n  manifest_dir: /path/to/dir\n  delete_after: 604800\n  debug: true\n</code></pre> <p>The <code>service</code> section contains parameters that control nuts-and-bolts behavior of the web service portion of the Data Transfer Service. The fields in this section are:</p> <ul> <li><code>port</code>: the port on which the service listens</li> <li><code>max_connections</code>: the maximum number of connections that are simultaneously   available for DTS clients. If a client sends a request to the DTS when all   connections are occupied, the request is denied.</li> <li><code>poll_interval</code>: the interval (in milliseconds) at which the DTS checks for   progress in any ongoing transfers. Because the file transfers orchestrated by   the DTS typically take a long time, it's reasonable to set this parameter to   a minute (60000 ms) or even longer. However, sometimes it's useful to have a   smaller polling interval, like when you're testing a feature. This parameter   is optional and defaults to 60000 ms.</li> <li><code>endpoint</code>: the name of an endpoint (defined in the endpoints   section) used by the DTS to generate and transfer manifests to destination   endpoints. This endpoint must have access to the file system to which the DTS   writes its manifests.</li> <li><code>data_dir</code>: a path to a directory on the local file system that the DTS uses   for its own storage. The DTS should have read/write access to this directory.</li> <li><code>manifest_dir</code>: a path to a directory on the local file system in which the   DTS writes transfer manifests. The endpoint named in the <code>endpoint</code> parameter   must have read access to this directory in order to send the manifest to its   destination.</li> <li><code>delete_after</code>: the interval (in seconds) after which the DTS deletes the   record for a completed transfer, whether the transfer completed successfully   or unsuccessfully. This makes it possible for users to query the status of   completed transfers for the given interval. This parameter is optional and   defaults to 7 days (604800 seconds).</li> <li><code>debug</code>: an optional parameter that, if set to <code>true</code>, enables more detailed   logging and other features that are helpful for troubleshooting and   development work. The default value is <code>false</code>.</li> </ul>"},{"location":"admin/config.html#endpoints","title":"<code>endpoints</code>","text":"<pre><code>endpoints:\n  globus-local:\n    name: name-of-local-endpoint\n    id: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n    provider: globus\n    auth:\n      client_id: &lt;ID of client with authentication secret&gt;\n      client_secret: &lt;secret&gt;\n  globus-jdp:\n    name: name-of-jdp-endpoint\n    id: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n    provider: globus\n    auth:\n      client_id: &lt;ID of client with authentication secret&gt;\n      client_secret: &lt;secret&gt;\n  globus-kbase:\n    name: name-of-kbase-endpoint\n    id: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n    provider: globus\n    auth:\n      client_id: &lt;ID of client with authentication secret&gt;\n      client_secret: &lt;secret&gt;\n</code></pre> <p>This section is a mapping (set of key-value pairs) that associates the names of endpoints (keys) with sets of parameters that define their behaviors (values). The endpoints defined here can be referred to in the other sections. The fields that define the behavior of each endpoint are:</p> <ul> <li><code>name</code>: a human-readable name for the endpoint, which can be helpful in   diagnostic and error-related messages</li> <li><code>id</code>: a UUID that uniquely identifies the endpoint in a (provider-specific)   way that allows the DTS to access it</li> <li><code>provider</code>: the name of the service providing the endpoint capability.   Valid values for this parameter are:<ul> <li><code>globus</code>: identifies the endpoint as a Globus Collection (in which case   the <code>id</code> parameter is the corresponding UUID)</li> <li><code>local</code>: identifies the endpoint as a local endpoint with access only to   the DTS's local file system. This type of endpoint is only useful for   testing.</li> </ul> </li> <li><code>auth</code>: this optional parameter provides authentication information to the   endpoint's provider if necessary. Its fields are:<ul> <li><code>client_id</code>: an ID that identifies the DTS to the endpoint's provider as   a client</li> <li><code>client_secret</code>: a string containing a secret corresponding to the ID   provided by the <code>client_id</code> parameter</li> </ul> </li> <li><code>root</code>: this optional parameter specifies the root directory used by DTS to   refer to files on the underlying filesystem of the endpoint. If left blank,   the root directory is set to <code>/</code>.</li> </ul>"},{"location":"admin/config.html#databases","title":"<code>databases</code>","text":"<pre><code>databases:\n  jdp:\n    name: JGI Data Portal\n    organization: Joint Genome Institute\n    endpoint: globus-jdp\n  kbase:\n    name: KBase Workspace Service (KSS)\n    organization: KBase\n    endpoint: globus-kbase\n</code></pre> <p>This section is a mapping (set of key-value pairs) that associates the names of databases (keys) with sets of parameters that define the databases themselves (at least, as far as the DTS is concerned). These databases are the sources and destinations for all file transfers performed by the DTS. The keys in this section identify the databases that are configured for the DTS, and are referred to in transfer requests specified by DTS clients. Supported databases are:</p> <ul> <li><code>jdp</code>: the Joint Genome Institute Data Portal</li> <li><code>kbase</code>: the Department of Energy Systems Biology Knowledgebase (KBase)</li> </ul> <p>Valid fields for each database are:</p> <ul> <li><code>name</code>: a human-readable name for the database, useful in diagnostic and   error-related messages</li> <li><code>organization</code>: a human-readable name for the organization that provides the   database (again, purely informational)</li> <li><code>endpoint</code>: the name of the endpoint defined in the endpoints   section that provides the DTS with access to the file staging area for the   database</li> </ul>"},{"location":"admin/deployment.html","title":"Deploying the DTS via Docker","text":"<p>You can use the <code>Dockerfile</code> and <code>dts.yaml</code> files in the <code>deployment</code> folder to build a Docker image for the Data Transfer System (DTS). The Docker image contains two files:</p> <ol> <li><code>/bin/dts</code>: the statically-linked <code>dts</code> executable</li> <li><code>/etc/dts.yaml</code>: a DTS configuration file with embedded    environment variables that control parameters of interest</li> </ol> <p>This image can be deployed in any Docker-friendly environment. The use of environment variables in the configuration file allows you to configure the DTS without regenerating the image.</p>"},{"location":"admin/deployment.html#deploying-to-nerscs-spin-environment","title":"Deploying to NERSC's Spin Environment","text":"<p>The \"primary instance\" of the DTS is hosted in NERSC's Spin environment under Rancher 2. It runs in the <code>Production</code> environment under the <code>kbase</code> organization. You can read about Spin in NERSC's documentation, and Rancher 2 here. The documentation isn't great, but fortunately there's not a lot to know--most of the materials you'll need are right here in the <code>deployment</code> folder.</p> <p>Deploying the DTS to Spin involves</p> <ol> <li>updating and pushing a new Docker image with any code changes and    documentation updates</li> <li>editing the <code>dts</code> Spin deployment via NERSC's    Rancher 2 console</li> </ol> <p>Each of these steps are fairly simple.</p> <p>Before you perform an update, take some time to familiarize yourself with the Rancher 2 console and the <code>dts</code> production deployment. The most important details are:</p> <ul> <li>The service and its supporting executables and configuration data are   supplied by its Docker image</li> <li>Configurable settings for the service are stored in environment variables   that can be set in the Rancher 2 console</li> <li>The DTS data directory (used for keeping track of ongoing tasks and for   generating transfer manifests) resides on the NERSC Community File System   (CFS) under <code>/global/cfs/cdirs/kbase/dts/</code>. This volume is visible to the   service as <code>/data</code>, so the <code>DATA_DIRECTORY</code> environment variable should be   set to <code>/data</code>.</li> <li>The DTS manifest directory (used for writing transfer manifest files that   get transferred to destination endpoints) also resides on the NERSC   Community File System (CFS), but under <code>/global/cfs/cdirs/kbase/gsharing/dts/manifests</code>   so that it is accessible via a Globus endpoint. This volume is visible to   the service as <code>/manifests</code>, so the <code>MANIFEST_DIRECTORY</code> environment variable   should be set to <code>/manifests</code>. NOTE: the directory must be the same when   viewed by the service and the Globus Collection! If there is a mismatch,   the service will not be able to write the manifest OR Globus will not be   able to transfer it.</li> </ul> <p>Let's walk through the process of updating and redeploying the DTS in Spin.</p>"},{"location":"admin/deployment.html#1-update-and-push-a-new-docker-image-to-spin","title":"1. Update and Push a New Docker Image to Spin","text":"<p>From within a clone of the DTS GitHub repo, make sure the repo is up to date by typing <code>git pull</code> in the <code>main</code> branch.</p> <p>Then, sitting in the top-level <code>dts</code> source folder of your <code>dts</code>, execute the <code>deploy-to-spin.sh</code> script, passing as arguments</p> <ol> <li>the name of a tag to identify the new Docker image</li> <li>the name of the NERSC user whose permissions are used for CFS</li> <li>the UID of the NERSC user</li> <li>the group used to determine the user's group permissions</li> <li>the GID of the above group</li> </ol> <p>For example,</p> <pre><code>./deployment/deploy-to-spin.sh v1.1 johnson 52710 kbase 54643\n</code></pre> <p>builds a new DTS Docker image for to be run as the user <code>johnson</code>, with the tag <code>v1.1</code>. The script pushes the Docker image to Harbor, the NERSC Docker registry. Make sure the tag indicates the current version of <code>dts</code> (e.g. <code>v1.1</code>) for clarity.</p> <p>After building the Docker image and tagging it, the script prompts you for the NERSC password for the user you specified. This allows it to push the image to Harbor so it can be accessed via the Rancher 2 console.</p>"},{"location":"admin/deployment.html#2-edit-the-deployment-in-rancher-2-and-restart-the-service","title":"2. Edit the Deployment in Rancher 2 and Restart the Service","text":"<p>Now log in to Rancher 2 and navigate to the <code>dts</code> deployment.</p> <ol> <li>Click on the <code>dts</code> pod to view its status and information</li> <li>Click on the three dots near the right side of the screen and select    <code>Edit</code> to update its configuration.</li> <li>If needed, navigate to the <code>Volumes</code> section and edit the CFS directory for    the volume mounted at <code>/data</code>. Usually, this is set to <code>/global/cfs/cdirs/kbase/dts/</code>,    so you usually don't need to edit this. Similarly, check the volume mounted    at <code>/manifests</code> (usually set to <code>/global/cfs/cdirs/kbase/gsharing/manifests/</code>).</li> <li>Edit the Docker image for the deployment, changing the tag after the colon    to match the tag of the Docker image pushed by <code>deploy-to-spin.sh</code> above.</li> <li>Make sure that the Scaling/Upgrade Policy on the Deployment is set to    <code>Recreate: KILL ALL pods, then start new pods.</code> This ensures that the    service in the existing pod can save a record of its ongoing tasks before a    service in a new pod tries to restore them.</li> <li>Click <code>Save</code> to restart the deployment with this new information.</li> </ol> <p>That's it! You've now updated the service with new features and bugfixes.</p>"},{"location":"admin/globus.html","title":"Granting the DTS Access to a Globus Endpoint","text":"<p>The Data Transfer Service (DTS) relies heavily on Globus for performing file transfers between different databases. Globus is an elaborate and continuously evolving platform, so configuring access from an application can be confusing. Here we describe all the things you need to know to grant the DTS access to a Globus endpoint.</p>"},{"location":"admin/globus.html#globus-glossary","title":"Globus Glossary","text":"<p>Globus has its own set of terminology that is slightly different from that we've  used to describe the DTS, so let's clarify some definitions first.</p> <ul> <li>Globus Endpoint: A Globus endpoint is a server running Globus software,   providing access to a filesystem that can be shared with Globus users. To the   DTS, an endpoint is \"a thing that can send and receive files.\"</li> <li>Globus Collection: A Globus Collection is a portion of a filesystem on a   Globus Endpoint associated with roles and permissions for Globus users. It is   not a server--it's a set of metadata that tells Globus which users have what   access privileges on a Globus Endpoint.</li> <li>Globus Guest Collection: A Guest Collection is a Globus Collection that allows   a Globus user to share files on with other Globus users and with applications.   In particular, a Guest Collection is the only mechanism that can provide   client applications with access to resources on Globus endpoints. This is the   closest concept to what the DTS considers an endpoint.</li> </ul>"},{"location":"admin/globus.html#setting-up-access-to-a-globus-endpoint","title":"Setting up Access to a Globus Endpoint","text":"<p>This guide gives a complete set of instructions using the terminology above. Below, we briefly summarize the steps in the guide. Of course, you need a Globus user account to play this game.</p> <ol> <li> <p>Obtain an Application/Service Credential for the DTS. The credential    consists of a unique client ID and an associated client secret. The client ID    can be used to identify the DTS as an entity that can be granted access    permissions. Of course, the primary instance of the DTS already has one of    these.</p> </li> <li> <p>Create a Guest Collection on the Globus Endpoint or on an existing Collection.    Without a Guest Collection, you can't grant the DTS access to anything. You might    have to poke around a bit to find an endpoint or existing collection that (a) you    have access to and (b) that exposes the resources that you want to grant to the    DTS.</p> </li> <li> <p>Grant the DTS read or read/write access to the Guest Collection. Since    the DTS has its own client ID, you can grant it access to a Guest Collection    just as you would any other Globus user.</p> </li> </ol> <p>The DTS stores its Globus credentials (client ID, client secret) in environment variables to prevent them from being read from a file or mined from the executable. The deployment section describes how these environment variables are managed in practice.</p>"},{"location":"admin/installation.html","title":"Installing the DTS Locally","text":"<p>Here we describe how to build, test, and install the Data Transfer Service (DTS) in a local environment.</p>"},{"location":"admin/installation.html#building-and-testing","title":"Building and Testing","text":"<p>The DTS is written in Go, so you'll need a working Go compiler to build, test, and run it locally. If you have a Go compiler, you can clone this repository and build it from the top-level directory:</p> <pre><code>go build\n</code></pre>"},{"location":"admin/installation.html#running-unit-tests","title":"Running Unit Tests","text":"<p>The DTS comes with several unit tests that demonstrate its capabilities, and you can run these tests as you would any other Go project:</p> <pre><code>go test ./...\n</code></pre> <p>You can add a <code>-v</code> flag to see output from the tests.</p> <p>Because the DTS is primarily an orchestrator of network resources, its unit tests must be able to connect to and utilize these resources. Accordingly, you must set the following environment variables to make sure DTS can do what it needs to do:</p> <ul> <li><code>DTS_KBASE_DEV_TOKEN</code>: a developer token for the KBase production   environment (available to KBase developers   used to connect to the KBase Auth Server, which provides a context for   authenticating and authorizing the DTS for its basic operations. You can create   a token from your KBase developer account.</li> <li><code>DTS_KBASE_TEST_ORCID</code>: an ORCID identifier that can be   used to run the DTS's unit test. This identifier must match a registered ORCID ID   associated with a KBase user account.</li> <li><code>DTS_KBASE_TEST_USER</code>: the KBase user associated with the ORCID specified   by <code>DTS_KBASE_TEST_ORCID</code>. NOTE: at the time of writing, KBase does not have   a mechanism for mapping ORCID IDs to local users, so the DTS uses a file in   its data directory called <code>kbase_users.json</code> consisting of a single JSON   object whose keys are ORCID IDs and whose values are local usernames.</li> <li><code>DTS_GLOBUS_CLIENT_ID</code>: a client ID registered using the   Globus Developers   web interface. This ID must be registered specifically for an instance of   the DTS.</li> <li><code>DTS_GLOBUS_CLIENT_SECRET</code>: a client secret associated with the client ID   specified by <code>DTS_GLOBUS_CLIENT_ID</code></li> <li><code>DTS_GLOBUS_TEST_ENDPOINT</code>: a Globus endpoint used to test the DTS's transfer   capabilities</li> <li><code>DTS_JDP_SECRET</code>: a string containing a shared secret that allows the DTS to   authenticate with the JGI Data Portal</li> </ul>"},{"location":"admin/installation.html#installation","title":"Installation","text":"<p>The only remaining step is to copy the <code>dts</code> executable from your source directory to wherever you want it to reside. This executable is statically linked against all libraries, so it's completely portable.</p>"},{"location":"developer/index.html","title":"DTS Developer Guide","text":"<p>This guide explains the internal workings of the Data Transfer Service (DTS).</p>"},{"location":"developer/index.html#code-organization","title":"Code Organization","text":"<p>The following packages implement the features in the Data Transfer Service.</p> <ul> <li>auth: handles the authorization of the DTS using KBase's   authentication/authorization server</li> <li>config: handles the parsing of the DTS YAML configuration   file, placing the data into read-only global variables   for use by other packages</li> <li>credit: defines metadata types used by the Credit Engine to   establish the provenance of transferred data</li> <li>databases: defines database types that implement the   integration of DTS with database providers</li> <li>endpoints: defines endpoint types for file transfer   providers used by DTS, such as Globus</li> <li>frictionless: defines data structures   that describe data for individual files   and packages containing multiple files</li> <li>services: defines types that implement the REST endpoints   provided by the DTS</li> <li>tasks: implements the \"heart\" of the DTS, which creates and   manages transfer tasks through their entire lifecycle</li> </ul>"},{"location":"developer/auth.html","title":"The <code>auth</code> Package","text":"<p>TODO: stuff goes here.</p>"},{"location":"developer/config.html","title":"The <code>config</code> Package","text":"<p>TODO: stuff goes here.</p>"},{"location":"developer/credit.html","title":"The <code>credit</code> Package","text":"<p>TODO: stuff goes here.</p>"},{"location":"developer/databases.html","title":"The <code>databases</code> Package","text":"<p>TODO: stuff goes here.</p>"},{"location":"developer/endpoints.html","title":"The <code>endpoints</code> Package","text":"<p>TODO: stuff goes here.</p>"},{"location":"developer/frictionless.html","title":"The <code>frictionless</code> Package","text":"<p>TODO: stuff goes here.</p>"},{"location":"developer/services.html","title":"The <code>services</code> Package","text":"<p>TODO: stuff goes here.</p>"},{"location":"developer/tasks.html","title":"The <code>tasks</code> Package","text":"<p>TODO: stuff goes here.</p>"},{"location":"integration/index.html","title":"DTS Integration Guide","text":"<p>This document lists all of the components that the Data Transfer System (DTS) expects from any database with which it interacts. Your organization must implement each of these components in order to integrate its database(s) with the DTS in order to take advantage of its file and metadata transfer capabilities.</p> <p>We have tried to cover all the necessary topics comprehensively here, but there's no substitute for a real person when confusion arises, so please don't hesitate to contact the KBase DTS development team with your questions. Take a look at the DTS Integration Glossary for an explanation of the terminology used in this guide.</p> <p>The guidance we present here is not intended to be prescriptive. We provide suggestions and examples of technical components to illustrate how your organization can integrate a database with the DTS, but in actuality the DTS is designed to be flexible and can accommodate various implementations. For example, we may be able to adapt existing capabilities for DTS integration in certain situations.</p>"},{"location":"integration/index.html#overview","title":"Overview","text":"<p>The DTS provides a file transfer capability whose organizational unit is individual files. We're not in the business of telling researchers how to do their jobs, and everyone in the business knows how to use a filesystem. If your organization's data is stored directly in a database and not as files, the DTS team can work with you to find the most appropriate way to write data to files upon request for transfer.</p> <p>If you're reading this, you're probably interested in making your data available to the DTS, and/or being able to receive data from other participating databases. How exactly does the DTS communicate with these databases? Here's what the DTS needs to navigate your organization's database.</p> <ol> <li>Every file (resource) in the database has a unique identifier. The    identifier can be any string (including a sequence of digits), as long as    that string refers to exactly one file. The DTS prepends an abbreviation for    your organization or database to the string to create its own namespaced    unique identifier. For example, JGI's Data Portal (JDP) has a file with the    identifier <code>615a383dcc4ff44f36ca5ba2</code>, and the DTS refers to this file as    <code>JDP:615a383dcc4ff44f36ca5ba2</code>.</li> <li>Your database can provide information about a file (resource) given its    unique identifier. Specifically, the database provides a resources    endpoint that accepts an HTTP request with a list of file IDs, and    provides a response containing essential informat\u0456on (the file's location,    its type and other important metadata) for each of the corresponding files.</li> <li>Given a search query, your database can identify matching files and    return a list of IDs for these files. In other words, the database provides    a search endpoint that accepts an HTTP request with a query string,    and produces a response containing a list of matching file IDs. This endpoint    allows a DTS user to select a set of files expediently.</li> <li>Your database must provide a staging area visible to a supported file    transfer provider, such as Globus. The DTS coordinates file transfers, but    does not handle the transfer of data by itself. For this, it relies on    commercial providers like Globus,    Amazon S3, and iRods.    In order for the DTS to be able to transfer your organization's data, you    must make a staging area available for transferred files that is visible    to one of these providers.</li> <li>If necessary, your database can move requested files (resources) to its    staging area where the DTS can access them for transfer. If your    organization archives data to long-term storage (tapes, usually), the DTS    needs to be able to request that this data be restored to a staging area    before it can get at them. Your database must provide a staging endpoint    that accepts an HTTP request with a list of resource IDs and returns    a UUID that can be used to query the status of the staging task.    Additionally, your database must provide a staging status endpoint that    accepts an HTTP request with a staging request UUID and produces a    response that indicates whether the staging process has completed.</li> <li>Your database can map ORCID IDs to local users within your organization.    Every DTS user must authenticate with an ORCID ID to connect to the service.    To establish a connection between the ORCID ID and a specific user account    for your organization, your database must provide a user federation    endpoint that accepts an HTTP request with an ORCID ID and produces    a response containing the corresponding username for an account within your    system. This federation process allows DTS to associate a transfer operation    with user accounts in the organizations for the source and destination    databases.</li> </ol> <p>Item 1 is entirely a matter of policy enforced within your organization. The other items have technical components which are discussed in more detail in the sections below.</p> <p>If your organization has existing services that provide similar capabilities but use different conventions, or if you have other technical considerations, please contact the DTS team to discuss how we can make the best of what you have.</p>"},{"location":"integration/index.html#contents","title":"Contents","text":"<ul> <li>Provide Unique IDs and Metadata for Your Files</li> <li>Make Your Files Searchable</li> <li>Provide a Staging Area for Your Files</li> <li>Stage Your Files on Request</li> <li>Provide a Way to Monitor File Staging</li> <li>Map ORCID IDs to Local User Accounts</li> </ul>"},{"location":"integration/glossary.html","title":"DTS Integration Glossary","text":"<p>We use the following terms in the DTS Integration Guide.</p> <ul> <li>DTS: The Data Transfer System</li> <li>Frictionless DataPackage: A collection of metadata   in the form of an array of Frictionless DataResources. The DTS generates   transfer manifests in this format.</li> <li>Frictionless DataResource: A representation   of metadata for an individual resource (file)</li> <li>Resource: A file, with a unique identifier, that can be transferred from   one database to another, along with its metadata.</li> <li>Resource endpoint: an endpoint provided by your database that accepts an   HTTP <code>GET</code> request with a list of file IDs, and provides a response containing   essential informat\u0456on (the file's location, its type and other important   metadata) for each of the corresponding files. Described in detail in   Provide Unique IDs and Metadata for Your Files.</li> <li>Search endpoint: an endpoint provided by your database that accepts an   HTTP <code>GET</code> request with a query string, and produces a response containing a   list of matching file IDs. See Make Your Files Searchable for   more detailed information.</li> <li>Staging area: A filesystem or portion of a filesystem on a system   controlled or provisioned by your organization where you can place files   for transfer by a bulk transfer provider such as Globus,   Amazon S3, or iRods.</li> <li>Staging endpoint: An endpoint provided by your database that accepts an   HTTP <code>POST</code> request with a list of resource IDs and returns a UUID that can   be used to query the status of a file staging operation. More information is   available in Stage Your Files on Request.</li> <li>Staging task: A process by which files with specific IDs can be   copied into your organization's staging area in preparation for transfer.   This process can occur over a brief or extended period, depending on the   number of requested files and the prevailing circumstances, so a staging   task is assigned a UUID that can be used to query its status.</li> <li>Staging status endpoint: An endpoint provided by your database that   accepts an HTTP <code>GET</code> request with a staging request UUID (obtained from a   staging endpoint), producing a response that indicates whether the staging   process has completed. See Provide a Way to Monitor File Staging   for more information.</li> <li>Transfer Manifest: a file (usually named <code>manifest.json</code>) containing a   machine-readable Frictionless DataPackage containing metadata for a set of   files transferred by the DTS. The DTS deposits a transfer manifest at the top   level of the directory structure transferred to the destination database.   a source database to a destination database by the DTS</li> <li>User federation endpoint: An endpoint provided by your database that   accepts an HTTP <code>GET</code> request with an ORCID ID and produces a response   containing the corresponding username for an account within your system.   Read more about this endpoint in Map ORCID IDs to Local User Accounts.</li> <li>UUID: A universally unique identifier   used by the DTS to represent staging tasks, transfers, and other transient-   but-possibly-long-running operations</li> </ul>"},{"location":"integration/local_user.html","title":"Map ORCID IDs to Local User Accounts","text":"<p>The Data Transfer System (DTS) uses ORCID IDs to identify individuals and organizations. In order to understand who is transferring what where when, your database must establish a connection between a user's ORCID ID and their local account on your system. This connection is a form of federated identity management, similar to Single Sign-On (SSO) authentication services offered by various platforms.</p>"},{"location":"integration/local_user.html#endpoint-recommendations","title":"Endpoint Recommendations","text":"<p>Create a REST endpoint that accepts an HTTP <code>GET</code> request with a given ORCID ID as a path parameter or request parameter. This endpoint responds with a body containing a JSON object with two fields:</p> <ul> <li><code>orcid</code>: the ORCID ID passed as the query parameter</li> <li><code>username</code>: the local username corresponding to the given ORCID ID</li> </ul> <p>Error codes should be used in accordance with HTTP conventions:</p> <ul> <li>A successful query returns a <code>200 OK</code> status code</li> <li>An improperly-formed ORCID ID should result in a <code>400 Bad Request</code> status code</li> <li>An ORCID ID that does not correspond to a local user should produce <code>404 Not Found</code></li> </ul>"},{"location":"integration/local_user.html#example","title":"Example","text":"<p>For example, suppose we want to find the local username for Josiah Carberry, a fictitious Professor of Psychoceramics at Brown University. The University's federated identity endpoint provides the endpoint <code>https://example.com/dts/localuser/{orcid}</code> that accepts a path parameter <code>{orcid}</code>. We can retrieve Josiah's local username in the University's database by sending an HTTP <code>GET</code> request to</p> <pre><code>https://example.com/dts/localuser/0000-0002-1825-0097\n</code></pre> <p>This produces a respon\u0455e with a <code>200 OK</code> status code with the body</p> <pre><code>{\n  \"orcid\": \"0000-0002-1825-0097\",\n  \"username\": \"psyclay\"\n}\n</code></pre>"},{"location":"integration/local_user.html#existing-implementations","title":"Existing implementations","text":"<p>This particular feature is actually not yet supported by KBase, so the DTS prototype uses a locally-stored JSON file containing an object whose field names are ORCID IDs and whose values are usernames for the corresponding KBase users.</p> <ul> <li>Example: current workaround (to be updated when JDP and KBase support this!)</li> </ul>"},{"location":"integration/resources.html","title":"Provide Unique IDs and Metadata for Your Files","text":"<p>If you're reading this, you probably belong to an organization that maintains a database containing many files of different sorts, and you probably want to make it easier for researchers to access (and cite!) the data in these files.</p> <p>In order for a file to be made available to users of BER's Data Transfer System (DTS), it must have a unique identifier by which users can refer to it. The file's name is probably not unique, so it's not an appropriate identifier. Unique file identifiers are such a basic need that your organization probably already has its own set.</p> <p>Any string containing a unique sequence of characters can serve as a unique file identifier for the DTS. For example, the following identifiers are all technically valid:</p> <ul> <li><code>615a383dcc4ff44f36ca5ba2</code></li> <li><code>machine-generated-id-21242452</code></li> <li><code>icanseemyhousefromhere</code></li> </ul> <p>The important thing is that the file can be uniquely identified. Users won't be typing these identifiers in manually--they'll typically obtain them by searching your database and selecting desired results.</p> <p>The DTS assigns a database specific prefix to your identifier that gives it a namespace inside which it is unique. For example, the JGI Data Portal identifier <code>615a383dcc4ff44f36ca5ba2</code> is made available by the DTS as <code>JDP:615a383dcc4ff44f36ca5ba2</code>.</p> <p>Sometimes a database consists of more than one dataset, and each dataset has its own identifiers. For example, Uniprot, one of the world's largest collections of protein sequences, defines unique identifiers for each of several distinct datasets. This link illustrates some examples of these dataset-specific identifiers.</p> <p>Because Uniprot's identifiers indicate which dataset they're from, all of the identifiers could be used within a single namespace by the DTS. This is one way of providing access to multiple datasets. If this method of aggregating datasets into a single namespace doesn't work for your organization, the DTS team is happy to discuss alternatives with you.</p>"},{"location":"integration/resources.html#file-metadata","title":"File Metadata","text":"<p>Whatever file identification scheme your organization uses, you can provide the DTS with the file information it needs by creating an endpoint that returns file metadata for a given unique file identifier.</p> <p>What's \"metadata\"? There is no universal answer to this question, of course. The DTS is part of an effort to harmonize how our community searches for, obtains, and cites scientific data, but the size and diversity of this community make it difficult to find an answer even for our purposes alone!</p> <p>Nevertheless, we press on. The metadata specification used by the DTS is likely to undergo changes of all sorts as we figure things out, but we'll give our best effort to updating documentation as these changes occur.</p>"},{"location":"integration/resources.html#dts-metadata-specification","title":"DTS Metadata Specification","text":"<p>The DTS stores file metadata in the Frictionless DataResource format. This format is simple and relatively unopinionated, and allows for additional fields as needed. The most important elements of the Frictionless DataResource type are:</p> <ul> <li><code>name</code>: the name of a resource (file)</li> <li><code>path</code>: the path of the resource relative to the root directory of the   staging area in which you make the file available</li> </ul> <p>The following additional fields are used by the DTS:</p> <ul> <li><code>id</code>: your organization's unique identifier for the resource</li> <li><code>credit</code>: credit metadata associated with the resource that conforms to the   KBase credit metadata schema</li> <li><code>metadata</code>: an optional un\u0455tructured field that you can use to stash   additional information about the resource if needed. For now, the DTS does not   use this field.</li> </ul> <p>It might be good to show an example here.</p> <p>By the way, the DTS uses the Frictionless DataPackage format to store file manifests for bulk file transfers. A data package is just a collection of data resources with some additional metadata that applies to all of them. A file manifest is generated automatically by the DTS after each successful transfer.</p> <p>If you adopt the Frictionless DataResource format for your own file metadata, integration with the DTS will be very easy. If your organization already has its own metadata format, the DTS team can work with you to determine how it can be translated to the Frictionless format for use by the DTS.</p>"},{"location":"integration/resources.html#endpoint-recommendations","title":"Endpoint Recommendations","text":"<p>Create a REST endpoint that accepts an HTTP <code>GET</code> request with a list of unique file identifiers specified (for example) by a comma-separated set of request parameters. This endpoint responds with a body containing a JSON list of objects representing Frictionless DataResources describing the requested files in as much detail as is practical.</p> <p>Error codes should be used in accordance with HTTP conventions:</p> <ul> <li>A successful query returns a <code>200 OK</code> status code</li> <li>An improperly-formed request should result in a <code>400 Bad Request</code> status code</li> <li>If one or more file IDs do not correspond to existing files in your   organization's database, their entries in the JSON list in the response should   be set to <code>null</code>.</li> </ul>"},{"location":"integration/resources.html#example","title":"Example","text":"<p>Suppose we want to retrieve metadata for a couple of files provided by the JGI Data Portal (JDP) with the unique identifiers <code>615a383dcc4ff44f36ca5ba2</code> and <code>61412246cc4ff44f36c8913f</code> (referred to by the DTS as <code>JDP:615a383dcc4ff44f36ca5ba2</code> and <code>JDP:61412246cc4ff44f36c8913f</code>, respectively). If the JDP endpoint is hosted at <code>example.com</code> and is implemented according to our recommendations, we can send the following <code>GET</code> request:</p> <pre><code>https://example.com/dts/resources?ids=615a383dcc4ff44f36ca5ba2,61412246cc4ff44f36c8913f\n</code></pre> <p>This results in a response with a <code>200 OK</code> status code with the body</p> <pre><code>[\n  {\n    \"id\": \"JDP:615a383dcc4ff44f36ca5ba2\",\n    \"name\": \"3300047546_18\",\n    \"path\": \"img/submissions/255985/3300047546_18.tar.gz\",\n    \"format\": \"tar\",\n    \"media_type\": \"application/x-tar\",\n    \"bytes\": 1455012,\n    \"hash\": \"148827a6c3da2eaa4188b931ae0edc15\",\n    \"sources\": [\n      {\n        \"title\": \"Wrighton, Kelly (Colorado State University, United States)\",\n        \"path\": \"https://doi.org/10.46936/10.25585/60001289\",\n        \"email\": \"kwrighton@gmail.com\"\n      }\n    ],\n    \"credit\": {\n      \"comment\": \"\",\n      \"description\": \"\",\n      \"identifier\": \"615a383dcc4ff44f36ca5ba2\",\n      \"license\": \"\",\n      \"resource_type\": \"dataset\",\n      \"version\": \"\",\n      \"contributors\": [\n        {\n          \"contributor_type\": \"Person\",\n          \"contributor_id\": \"\",\n          \"name\": \"Wrighton, Kelly\",\n          \"credit_name\": \"Wrighton, Kelly\",\n          \"affiliations\": [\n            {\n              \"organization_id\": \"\",\n              \"organization_name\": \"Colorado State University\"\n            }\n          ],\n          \"contributor_roles\": \"PI\"\n        }\n      ],\n      \"dates\": [\n        {\n          \"date\": \"2019-09-17\",\n          \"event\": \"approval\"\n        }\n      ],\n      \"funding\": null,\n      \"related_identifiers\": null,\n      \"repository\": {\n        \"organization_id\": \"\",\n        \"organization_name\": \"\"\n      },\n      \"titles\":null\n    }\n  },\n  {\n    \"id\": \"JDP:61412246cc4ff44f36c8913f\",\n    \"name\": \"3300047546\",\n    \"path\": \"img/submissions/255985/3300047546.tar.gz\",\n    \"format\": \"tar\",\n    \"media_type\": \"application/x-tar\",\n    \"bytes\": 709508155,\n    \"hash\": \"fefe05140b0cad4e5d525acb066439e5\",\n    \"sources\": [\n      {\n        \"title\":\"Wrighton, Kelly (Colorado State University, United States)\",\n        \"path\":\"https://doi.org/10.46936/10.25585/60001289\",\n        \"email\":\"kwrighton@gmail.com\"\n      }\n    ],\n    \"credit\": {\n      \"comment\": \"\",\n      \"description\": \"\",\n      \"identifier\": \"61412246cc4ff44f36c8913f\",\n      \"license\": \"\",\n      \"resource_type\": \"dataset\",\n      \"version\": \"\",\n      \"contributors\": [\n        {\n          \"contributor_type\": \"Person\",\n          \"contributor_id\": \"\",\n          \"name\":\"Wrighton, Kelly\",\n          \"credit_name\": \"Wrighton, Kelly\",\n          \"affiliations\": [\n            {\n              \"organization_id\": \"\",\n              \"organization_name\": \"Colorado State University\"\n            }\n          ],\n          \"contributor_roles\": \"PI\"\n        }\n      ],\n      \"dates\": [\n        {\n          \"date\": \"2019-09-17\",\n          \"event\": \"approval\"\n        }\n      ],\n      \"funding\":null,\n      \"related_identifiers\": null,\n      \"repository\": {\n        \"organization_id\": \"\",\n        \"organization_name\": \"\"\n      },\n      \"titles\":null\n    }\n  },\n]\n</code></pre> <p>As you can see, many fields are blank, because some information may not be present in the database. The DTS works with the KBase Credit Engine to fill in missing credit information.</p>"},{"location":"integration/resources.html#existing-implementations","title":"Existing implementations","text":"<p>As it happens, the JDP doesn't provide its own file metadata endpoint, so the DTS has its own logic for querying the JDP's source of truth. The DTS can bridge gaps in the capabilities of the databases of participating organizations in this way, making it easier for you to gradually implement a conforming capability.</p>"},{"location":"integration/search.html","title":"Make Your Files Searchable","text":"<p>If you've established unique identifiers for your organization's files and implemented an endpoint to provide unique IDs and metadata for them, your next step is to allow BER researchers to search for files of interest so they can select which ones they would like to move around.</p> <p>Search technology has been around a long time and has gotten pretty sophisticated, with diverse options to meet diverse needs. Some interesting options for implementing your own search capability are (in alphabetical order):</p> <ul> <li>Amazon CloudSearch</li> <li>Apache Solr</li> <li>Elasticsearch</li> <li>OpenSearch</li> <li>Meilisearch</li> <li>Postgres FTS</li> <li>Redis Search</li> <li>Typesense</li> </ul> <p>Your organization may use one or a few of these options already. Each of these capabilities has its own strengths and weaknesses, based on the purpose for which it was conceived and built. The structure and content of your database can help you determine which is most appropriate.</p> <p>The DTS does not currently embrace any one search engine at the expense of others. Our team can work with you to make sure that your integration exposes all the necessary features of your file search capability.</p>"},{"location":"integration/search.html#endpoint-recommendations","title":"Endpoint Recommendations","text":"<p>Create a REST endpoint that accepts an HTTP <code>GET</code> request with a set of request parameters providing</p> <ul> <li>an appropriate query string that your database can use to search for files</li> <li>pagination parameters that allow search results to be broken into sets of   manageable sizes for inspection by a human being, e.g.<ul> <li>the maximum number of results to return for a single request (\"results   per page\")</li> <li>the offset of the first result to display among all results (\"the starting   page\")</li> </ul> </li> <li>whatever other information your search engine may need to do its thing</li> </ul> <p>The endpoint responds with a body containing a JSON list of objects representing Frictionless DataResources describing the files that match the given search query in as much detail as is practical.</p> <p>Error codes should be used in accordance with HTTP conventions:</p> <ul> <li>A successful query returns a <code>200 OK</code> status code</li> <li>An improperly-formed request should result in a <code>400 Bad Request</code> status code</li> </ul>"},{"location":"integration/search.html#example","title":"Example","text":"<p>The JGI Data Portal (JDP) uses ElasticSearch to implement its file search capability.</p> <p>At the time of writing, this query produces well over 100 results. Here are the first two results, expressed as Frictionless DataResource objects in JSON:</p> <pre><code>[\n  {\n    \"id\": \"JDP:57f9e03f7ded5e3135bc069e\",\n    \"name\": \"10927.1.183804.CTCTCTA-AGGCTTA.QC\",\n    \"path\": \"rqc/10927.1.183804.CTCTCTA-AGGCTTA.QC.pdf\",\n    \"format\": \"pdf\",\n    \"bytes\": 227745,\n    \"hash\": \"71a60d25af7b35227e8b0f3428b49687\",\n    \"sources\": [\n      {\n        \"title\": \"Stewart, Frank (Georgia Institute of Technology, United States)\",\n        \"path\": \"https://doi.org/10.46936/10.25585/60000893\",\n        \"email\": \"frank.stewart@biology.gatech.edu\"\n      }\n    ],\n    \"credit\": {\n      \"comment\": \"\",\n      \"description\": \"\",\n      \"identifier\": \"JDP:57f9e03f7ded5e3135bc069e\",\n      \"license\": \"\",\n      \"resource_type\": \"dataset\",\n      \"version\": \"\",\n      \"contributors\": [\n        {\n          \"contributor_type\": \"Person\",\n          \"contributor_id\": \"\",\n          \"name\": \"Stewart, Frank\",\n          \"credit_name\": \"Stewart, Frank\",\n          \"affiliations\": [\n            {\n              \"organization_id\": \"\",\n              \"organization_name\": \"Georgia Institute of Technology\"\n            }\n          ],\n          \"contributor_roles\": \"PI\"\n        }\n      ],\n      \"dates\": [\n        {\n          \"date\": \"2013-09-20\",\n          \"event\": \"approval\"\n        }\n      ],\n      \"funding\": null,\n      \"related_identifiers\": null,\n      \"repository\": {\n        \"organization_id\": \"\",\n        \"organization_name\": \"\"\n      },\n      \"titles\": null\n    }\n  },\n  {\n    \"id\": \"JDP:57f9d2b57ded5e3135bc0612\",\n    \"name\": \"10927.1.183804.CTCTCTA-AGGCTTA.filter-SAG\",\n    \"path\": \"rqc/filtered_seq_unit/00/01/09/27/10927.1.183804.CTCTCTA-AGGCTTA.filter-SAG.fastq.gz\",\n    \"format\": \"fastq\",\n    \"media_type\": \"text/plain\",\n    \"bytes\":2225019092,\n    \"hash\": \"55715364087c8553c99c126231e599dc\",\n    \"sources\":[\n      {\n        \"title\": \"Stewart, Frank (Georgia Institute of Technology, United States)\",\n        \"path\": \"https://doi.org/10.46936/10.25585/60000893\",\n        \"email\": \"frank.stewart@biology.gatech.edu\"\n      }\n    ],\n    \"credit\": {\n      \"comment\": \"\",\n      \"description\": \"\",\n      \"identifier\": \"JDP:57f9d2b57ded5e3135bc0612\",\n      \"license\": \"\",\n      \"resource_type\": \"dataset\",\n      \"version\": \"\",\n      \"contributors\":[\n        {\n          \"contributor_type\": \"Person\",\n          \"contributor_id\": \"\",\n          \"name\": \"Stewart, Frank\",\n          \"credit_name\": \"Stewart, Frank\",\n          \"affiliations\": [\n            {\n              \"organization_id\": \"\",\n              \"organization_name\": \"Georgia Institute of Technology\"\n            }\n          ],\n          \"contributor_roles\": \"PI\"\n        }\n      ],\n      \"dates\": [\n        {\n          \"date\": \"2013-09-20\",\n          \"event\": \"approval\"\n        }\n      ],\n      \"funding\": null,\n      \"related_identifiers\":null,\n      \"repository\": {\n        \"organization_id\": \"\",\n        \"organization_name\": \"\"\n      },\n      \"titles\":null\n    }\n  },\n  ...\n]\n</code></pre> <p>Many fields are blank, particularly in the <code>credit</code> field, because some of this information isn't in the JDP database. The DTS works with the KBase Credit Engine to fill in missing credit information.</p>"},{"location":"integration/search.html#existing-implementations","title":"Existing implementations","text":"<p>The JDP search endpoint we've described in the above example returns information about the files matching a query, but not in the Frictionless format we've shown. The JDP actually organizes its search results into organisms, and the DTS unpacks these results and repackages them into Frictionless DataResource objects. This is another example of how the DTS team can support incremental integration by working with your organization.</p>"},{"location":"integration/stage_files.html","title":"Stage Your Files on Request","text":"<p>After you've designated a file staging area and configured a Globus guest collection for your organization, it's time to set up a staging request endpoint that allows the Data Transfer System (DTS) to ask for specific files to be moved to this staging area in preparation for transfer.</p> <p>Why is this endpoint needed? In a perfect world, storage would be infinite and free, and your organization could simply \"keep all the files somewhere, ready to be transferred anytime.\" In practice, the amount of scientific data being continuously generated greatly exceeds available storage at any given moment, so it needs to be archived when it's not being used.</p> <p>For example, the JGI Data Portal (JDP) stores most of its data on tape, unarchiving it to disk (the staging area used by the DTS, specifically) on request. Your organization probably has a similar scheme in place for storing its data.</p> <p>It works like this: when a user searches for and selects files to transfer from your database to somewhere else, the DTS checks your staging area for the requested files via Globus. If those files are not in your staging area, the DTS makes a request to the staging request endpoint to ask your system to copy the files into place, whatever that entails.</p> <p>The DTS understands that this process may take some time, so it also needs a way to request the status of your file staging operation. After all, there are only so many robot arms in a tape backup facility, and they can only swing around so quickly and move so many tapes at once. Accordingly, your system must provide a unique identifier for the file staging operation that allows the DTS to check for its completion.</p> <p>Let's take a look at the staging request endpoint. We'll discuss how your system can report the status of a file staging operation in the next section.</p>"},{"location":"integration/stage_files.html#endpoint-recommendations","title":"Endpoint Recommendations","text":"<p>Create a REST endpoint that accepts an HTTP <code>POST</code> request with a body that contains a set of unique identifiers corresponding to files that should be moved to your file staging area. The endpoint validates the request, checking that the files exist in your system, and initiate a staging operation that copies the requested files into place within your staging area. The endpoint then responds with a body containing a universally unique identifier (UUID) that the DTS can use to request the status of the staging operation.</p> <p>Error codes should be used in accordance with HTTP conventions:</p> <ul> <li>A successful query returns a <code>201 Created</code> status code</li> <li>An improperly-formed request should result in a <code>400 Bad Request</code> status code</li> <li>If one or more file IDs do not correspond to existing files in your   organization's database, the endpoint can respond with a <code>404 Not Found</code>   status code.</li> </ul>"},{"location":"integration/stage_files.html#example","title":"Example","text":"<p>Suppose we want to request that JGI Data Portal (JDP) stage the files with the unique identifiers <code>615a383dcc4ff44f36ca5ba2</code> and <code>61412246cc4ff44f36c8913f</code> (referred to by the DTS as <code>JDP:615a383dcc4ff44f36ca5ba2</code> and <code>JDP:61412246cc4ff44f36c8913f</code>, respectively). If the JDP endpoint is hosted at <code>example.com</code> and is implemented according to our recommendations, we can send a <code>POST</code> to</p> <pre><code>https://example.com/dts/staging\n</code></pre> <p>with the following body:</p> <pre><code>{\n  \"ids\": [\n    \"615a383dcc4ff44f36ca5ba2\",\n    \"61412246cc4ff44f36c8913f\"\n  ]\n}\n</code></pre> <p>This results in a response with a <code>201 Created</code> status code with a body containing a UUID:</p> <pre><code>{\n  \"request_id\": \"4b86e181-8c83-447e-aada-af9232af7da0\"\n}\n</code></pre> <p>We'll see how this UUID can be used to retrieve the status of the file staging operation in the next section.</p>"},{"location":"integration/stage_files.html#existing-implementations","title":"Existing implementations","text":"<p>The JDP endpoint we mention above essentially conforms to what we've described, but contains some additional fields that determine, for example, whether an email is sent upon completion of the staging process. This is because we've repurposed an endpoint that was originally intended to allow users to download requested files directly from Globus. If your organization already has something in place that can serve as a file staging request endpoint, we can work with you to similarly leverage it to get your system hooked up to the DTS.</p>"},{"location":"integration/staging_area.html","title":"Provide a Staging Area for Your Files","text":"<p>In previous sections we discussed how you can provide researchers with information about the files in your database, both in the small with file-specific metadata and in the large with a text-based search capability. Now we turn our attention to the process of allowing these users to access these files and transfer them to other locations for analysis and manipulation by site-specific processes.</p> <p>Recall that the Data Transfer System (DTS) orchestrates transfers of data between the databases of participating organizations like yours. Transferring such data invariably involves moving files from a \"source\" staging area in one organization to a \"destination\" staging area in another. Here, a staging area is a file system visible to some file transport mechanism such as Globus. To allow the DTS to access your data for transfer, your organization must establish and maintain such a file staging area.</p> <p>At this time, the DTS relies on Globus to move files between different databases and organizations. Globus is widely used in the scientific research community by universities and DOE national laboratories, and its design reflects its focus on helping researchers share and access data. Additionally, Globus can be integrated with other file storage and transfer platforms like Amazon S3, Google, and many others. This interoperability allows Globus clients to interoperate seamlessly even in heterogeneous technological settings.</p>"},{"location":"integration/staging_area.html#setting-up-a-staging-area","title":"Setting up a Staging Area","text":"<p>First, your organization must allocate a file staging area on a filesystem that you can expose to Globus. The directory structure of this staging area for your staged files isn't important to anyone outside your organization, as long as all of the files staged there can be made accessible to Globus (and transitively, to the DTS). Recall, though, that the <code>path</code> element in the Frictionless DataResource metadata specification for each file should match your staging area's directory structure--otherwise, the DTS won't be able to locate your file.</p> <p>Probably the most important decision you'll make about the staging area is the directory in the filesystem that serves as the effective \"root\" directory for the staging area. This \"root\" directory determines the root path you'll use when you create a Globus guest collection that allows the DTS to access its files.</p> <p>The DTS is a work in progress, and we haven't yet worked out all the details for handling private or embargoed datasets. We're interested to hear your opinions on this and other topics!</p>"},{"location":"integration/staging_area.html#exposing-your-staging-area-to-globus","title":"Exposing Your Staging Area to Globus","text":"<p>If your organization has a Globus subscription, you'll have an easy time configuring a file staging area that can be integrated with the DTS. Globus has extensive documentation to help you understand how to share your data effectively.</p> <p>To make your staging area available to the DTS, you'll need to set up a Globus guest collection. The following links can help you configure your Globus setup:</p> <ul> <li>How to Share Data Using Globus:   a step-by-step guide to creating a guest collection that can serve as a   staging area for your organization</li> <li>Storage Connector Usage Guides:   a landing page for documentation about Globus's connectors,   which allow you to access data stored on other platforms like Amazon S3 and   Google Cloud Storage</li> <li>The Modern Research Data Portal:   an article describing a design pattern for providing secure, scalable, and   high performance access to research data</li> <li>Globus Connect Server:   information about how Globus can be deployed within your organization   (probably most useful to an IT department!)</li> </ul> <p>In particular, the first link walks you through the steps of creating a guest collection. When you've set up a guest collection in which you can stage files for transmission and receipt, the DTS team can work with you to authorize the Data Transfer Service to use this guest collection so it can access these files.</p>"},{"location":"integration/staging_status.html","title":"Provide a Way to Monitor File Staging","text":"<p>In the last section, we described the file staging process, in which the Data Transfer System (DTS) asks your system to stage a set of files for transfer. The DTS makes a file staging request, and your system returns some sort of identifier that can be used to track the staging operation. In this section, we describe a staging status endpoint that the DTS can query using the unique staging identifier provided by the database.</p> <p>This one's easy--your endpoint should accept a UUID that the DTS was given when it requested a staging operation for a set of specific files, and return information about the status of that operation.</p>"},{"location":"integration/staging_status.html#endpoint-recommendations","title":"Endpoint Recommendations","text":"<p>Create a REST endpoint that accepts an HTTP <code>GET</code> request with a UUID string received from a prior <code>POST</code> request for a file staging operation. The endpoint responds with a body containing a <code>status</code> field with a string value conveying that the files are <code>ready</code>, or <code>staging</code> (for example).</p> <p>Error codes should be used in accordance with HTTP conventions:</p> <ul> <li>A successful query returns a <code>200 OK</code> status code</li> <li>An improperly-formed request should result in a <code>400 Bad Request</code> status code</li> <li>If the UUID provided in the request doesn't match any ongoing or completed   file staging operation, your endpoint should return a <code>404 Not Found</code> status   code.</li> </ul>"},{"location":"integration/staging_status.html#example","title":"Example","text":"<p>Suppose now that we want to retrieve the status for the JGI Data Portal file staging operation we requested in the last section (UUID: <code>4b86e181-8c83-447e-aada-af9232af7da0</code>). If the JDP endpoint is hosted at <code>example.com</code> and is implemented according to our recommendations, we can send the following <code>GET</code> request:</p> <pre><code>https://example.com/dts/staging?id=4b86e181-8c83-447e-aada-af9232af7da0\n</code></pre> <p>This results in a response with a <code>200 OK</code> status code with the body</p> <pre><code>{\n  \"status\": \"ready\"\n}\n</code></pre> <p>This response conveys to the DTS that the requested files have been copied into place within the file staging area, so their transfer can begin.</p>"},{"location":"integration/staging_status.html#existing-implementations","title":"Existing implementations","text":"<p>The JDP's file staging status endpoint returns a very elaborate set of status information, most of which is not used by the DTS. As we've discussed in previous sections, the DTS prototype implementation repurposes a lot of existing endpoints for the JDP, and we are prepared to work with your organization to determine how best to wire your existing system into the DTS.</p>"}]}